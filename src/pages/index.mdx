---
layout: ../layouts/Layout.astro
title: HocusPOCUS
description: A OOD benchmark for point-of-care-ultrasound 
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import hocuspocus from "../assets/hocuspocus.svg";
import applications from "../assets/applications.svg";
import fig2 from "../assets/fig2.svg";
import fig3 from "../assets/fig3.svg";
import supplemental_video from "../assets/supplemental_video.gif";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Stanley Bryan Zamora Hua",
      url: "https://stan-hua.github.io/",
      institution: "The Hospital for Sick Children",
    },
    {
      name: "Lauren Erdman",
      institution: "Cincinnati Children's Hospital",
    },
  ]}
  conference="Pre-Print"
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/stan-hua/hocuspocus",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
    {
      name: "Zenodo",
      url: "https://zenodo.org/records/14941380",
      icon: "ri:database-2-line",
    },
    {
      name: "Kaggle",
      url: "https://www.kaggle.com/datasets/stanleyhua/hocuspocus-an-ood-benchmark-for-pocus/",
      icon: "ri:database-2-line",
    }
  ]}
  />

## The HocusPOCUS Benchmark

|                                         | **Description**                                        | **Train/Val/Test**  |
|-----------------------------------------|--------------------------------------------------------|---------------------|
| **HP-Atlas** (854 videos; 48.7K images) | Crowd-sourced POCUS videos from 11 different use-cases | 176/217/461 videos  |
| **HP-Quality** (1050 image pairs)       | Pairs of POCUS vs. non-POCUS images                    | 426/520/1154 images |
| **HP-Noise** (1920 images)              | Generated images with different foreground shapes      | 480/480/960 images  |

<Figure>
    <Image source={hocuspocus} altText="HocusPOCUS Datasets" />
</Figure>

<HighlightedSection>

## TLDR;

Real-time machine learning for point-of-care ultrasound requires knowing ***when a model doesn't know*.
We introduce HocusPOCUS, a benchmark for detecting data shifts in realistic deployment scenarios.
Experiments on three clinical applications highlight the promise and existing limitations of using maximum logit score, baseline post-hoc method, to identify model failure under both semantic and covariate shift.
By addressing these challenges, OOD detection can enhance the reliability and trustworthiness of machine learning systems in dynamic real-world POCUS settings.
</HighlightedSection>


## üèÉ Quickstart

**Load HocusPOCUS Datasets:**
```python
from hocuspocus_ood import HocusPocusDataset, load_hocuspocus_metadata

# Parameters
dataset_name = "atlas"          # or "quality" or "noise"

# Optional parameters
hparams = {
    "transform": ...                # Torchvision transforms
    "load_image_func": ...,         # Custom calling function to load image given image path
    "load_image_kwargs": ...,       # Keyword arguments for custom image loading function
    "img_mode": 3,                  # 3 = RGB, 1 = Grayscale
    "img_size": (224, 224),         # Image size to resize to
    "scale": True,                  # If True, perform min-max normalization
}

# 1. HP-Atlas dataset               # NOTE: Both the extracted foreground and background are loaded
dataset = HocusPocusDataset("atlas", **hparams)
# 1.1. HP-Atlas dataset with background details kept
# dataset = HocusPocusDataset("atlas", separate_background=False, **hparams)
print(dataset[0])
# {
#   'id': 'pocus_atlas-renal-3-1',
#   'video_id': 'pocus_atlas-renal-3',
#   'img': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1569]]]),
#   'background_img': tensor([[[0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0078, 0.0000]]]),
#   'label': 'renal',
# }

# 2. HP-Quality dataset
dataset = HocusPocusDataset("quality", **hparams)
print(dataset[0])
# {
#   'id': '0001-0',
#   'img': tensor([[[0.4235, 0.4510, 0.5020,  ..., 0.4627, 0.4471, 0.4549]]]),
#   'label': 'thyroid',
#   'quality': 'low_quality',
# }

# 3. HP-Noise dataset
dataset = HocusPocusDataset("noise", **hparams)
print(dataset[0])
# {
#   'id': 'hp_noise-1',
#   'img': tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]]),
#   'label': 'noise-solid',
# }
```

---


## üë©üèª‚Äç‚öïÔ∏è Clinical Applications
**2D Ultrasound View Classification**:
- **Adult Knee** ([K-JoCo](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SKP9IB))
- **Maternal-Fetal** ([MF-Spain](https://zenodo.org/records/3904280), [MF-Africa](https://zenodo.org/records/7540448), MF-3 (Private))
- **Pediatric Renal** (PR-1 (Private), PR-2 (Private))

<Figure caption="">
    <Image source={applications} altText="Clinical Applications" />
</Figure>

---

## üö® Key Findings

### üí°OOD Detection Can Identify Model Failure under Semantic and Covariate Shif
<Image source={fig2} altText="OOD Detection Identifies Model Failure under Semantic and Covariate Shift" />
**OOD detection can identify correctly predicted examples under covariate shift, but such examples are not easily separable from misidentified unseen views.**

The x-axis plots negative MLS (higher = more OOD) with quartiles marked.
Vertical line display set OOD decision thresholds.
For seen views, green region = identified inliers (In) and red region = identified outliers (Out).
Accuracies are provided above each region. For unseen views, green/red regions = correctly/incorrectly identified as semantic ID or OOD.
In parenthesis is the dataset used; A1=`K-JoCo`, B1=`MF-Spain`, B2=`MF-Africa`, B3=`MF-3`, C1=`PR-1`, and C2=`PR-2`.


### ‚ö†Ô∏è Caveat 1. Disambiguating Failure between Semantic vs. Covariate Shift Detection is Manual and Requires Domain Expertise.
MF-3 fetal images are captured in the 1st trimester, compared to the others which are captured in the 2nd/3rd trimester. Due to drastic changes between 1st and 2nd/3rd trimester images, the shift in age is more akin to a semantic shift, and thus should not be evaluated by model performance. As it stands, differentiating between the two will require manual labor in practice.

### TODO: Show difference in image between 1st and 2nd/3rd trimester images

### ‚ö†Ô∏è Caveat 2. Detection Can Amplify Model Biases
<Image source={fig3} altText="OOD Detection Identifies Model Failure under Semantic and Covariate Shift" />
**OOD Detection Amplifies Biases.**

**(a)** More severe disease manifestations are identified as outliers (K-JoCo: osteoarthritis, PR-1/PR-2: hydronephrosis) despite little effect on model performance. Bars (left-axis) indicate percentage identified as outliers, while points (right-axis) indicate accuracy gap between inliers and outliers.

**(b)** OOD worsens label imbalance among identified inliers, particularly in the imbalanced setting. Smallest/Largest refer to the least/most represented class in each dataset.

**(c)** OOD can marginally worsen label balance in the balanced setting.


### ‚ö†Ô∏è Caveat 3. Detection is Affected by Spurious Scanner-Specific Covariates
<TwoColumns>
  <Figure slot="left" caption="Affected by Background">
    <Image source={supplemental_video} altText="[FILL HERE]" />
  </Figure>
  <Figure slot="right" caption="Affected by Foreground Shape">
    <Image source={supplemental_video} altText="[FILL HERE]" />
  </Figure>
</TwoColumns>


### ‚ö†Ô∏è Caveat 4. MLS is not ready for real-time OOD detection
<Figure caption="Example (Lung) POCUS Video">
  <Image source={supplemental_video} altText="Example (Lung) POCUS Video" />
</Figure>

<Table>
  | Metric       | Adult Knee | Maternal-Fetal | Pediatric Renal |
  |--------------|------------|----------------|-----------------|
  | AUROC (Max Intra-Video Score)    | 99%       | 75.8%          | 66.6%           |
  | AUROC (Min Intra-Video Score)    | 89.6%      | 48.7%          | 37.4%           |
</Table>

---

## BibTeX citation

```bibtex
@misc{...,
  author = "{Stanley Bryan Zamora Hua, ..., Lauren Erdman}",
  title = "HocusPOCUS: A OOD benchmark for point-of-care ultrasound",
  year = "2025",
  howpublished = "\url{...}",
}
```